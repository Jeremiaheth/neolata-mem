Practical implementation notes
Do a fresh code-aware alignment of function names/paths to “where it stands right now”. The PRD I wrote is therefore integration-accurate (triggers/boundaries/workflows), but API-name-agnostic where necessary (e.g., mem.search vs mem.context).
rewrite the PRD sections to match your exact method names and options.  docs/ "PRD-openclaw-neolata-mem-session-memory.md"

1) Auto-store on key moments

Don’t make this depend on an LLM call every turn.

A good MVP pattern:

Keep a tiny deterministic detector (regex + structural cues) to decide “should we store?”

Only when “yes” do you:

store a compact memory (and optionally call an LLM to format/extract)

Examples of deterministic “key moment” cues:

Assistant output contains headings like Decision:, Next steps:, We will…

User says “let’s go with…”, “ship it”, “we decided…”

A TODO is created/closed (if OpenClaw has task objects, that’s a strong signal)

2) HEARTBEAT idle snapshot

Two critical bits to avoid spam and junk:

Debounce + dirty bit: only snapshot if something meaningful happened since last snapshot.

Snapshot content uses already-extracted items (decisions/open threads) rather than trying to summarize raw chat every time.

If you do want an LLM summary here, keep it on a leash:

feed it only the “salient buffer” (the extracted decisions/open threads), not the full transcript

3) Contextual recall on startup

The easiest win is to stop thinking “top 15 recent” and instead do three channels:

recent(5)

semantic(seedText, ~8)

high-importance(>=8, ~10 or time-bounded)

Then dedupe and pack into a budget.

Even before you ship confidence scoring, this gets you 80% of the “feels smart” effect.

4) Pre-compaction dump

This is where people mess up by storing one giant blob.

Store:

N small structured memories (decisions, blockers, open threads)

plus one snapshot that ties them together

That yields better retrieval later and reduces noise.


----------------------------------------------------------
PRD-v0.8.1-nice-to-haves.md docs/

Extra “nice-to-haves” I’d add (beyond your 3)

You already picked great ones (predicate schema registry, explainability, quarantine). If you want a few more that directly improve agent experience:

Idempotent write keys

e.g., dedupeKey = sha1(sessionId + normalizedClaimKey + value)

prevents accidental double-store on retries / reconnects

Dry-run store mode

store(..., { dryRun: true }) → { wouldStore, conflicts, wouldSupersede }

super useful when tuning conflict rules / quarantine

Memory “diff” tool for the agent

“What changed in memory this session?” → helps debugging + trust

Per-topic token budgets

prevents one topic (“OCI”) from crowding out everything else in startup recall

Poisoning test harness

a small suite of adversarial inputs (prompt injection, bogus tool output, contradiction spam) + assertions on quarantine/supersession behavior

this pays back immediately once you start auto-store


-----------------------------------------------------------------------------------------
(This is the TDD prompt)

Draft a detailed, step-by-step blueprint for building this with 100% accuracy + efficiency. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move forward. Iterate until you feel that the steps are right sized for this.

From here you should have the foundation to provide a series of prompts for a code-generation (CODEX) that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, 100% accuracy and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.

Make sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.